{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613c6973-81e1-4b22-8a53-61b6c7a8ffa8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8155bb40-ebd3-467b-90a8-e8e815a147d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen abc>:106: LangGraphDeprecatedSinceV10: AgentStatePydantic has been moved to `langchain.agents`. Please update your import to `from langchain.agents import AgentStatePydantic`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# RAG & LangChain Imports\n",
    "import chromadb\n",
    "import langchain\n",
    "import langchainhub\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Evaluation Metrics Imports\n",
    "import krippendorff\n",
    "from bert_score import score as bert_score\n",
    "from ragas.metrics import answer_relevancy\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749286a-1690-4e5b-8f08-42afb623c5a5",
   "metadata": {},
   "source": [
    "### RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89171057-9465-4cd4-974b-06f4875f7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openAI_embed = OpenAIEmbeddings(model=\"text-embedding-3-large\") #\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"CleanCorpus.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=openAI_embed,\n",
    "    collection_name=\"plantCorpus_langchain\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "## Role and Goal\n",
    "You are an expert landscaping and agricultural assistant with deep, practical knowledge of the Indo-Gangetic plains. Your goal is to provide accurate, specific answers to user queries about plants.\n",
    "\n",
    "## Source of Truth: Plant Corpus\n",
    "You will be given context from a specialized `Plant Corpus`. This corpus is your **single source of truth** for all plant-specific data.\n",
    "\n",
    "### Retrieved Context:\n",
    "{corpus_content}\n",
    "\n",
    "## Core Instructions and Rules\n",
    "\n",
    "1.  **Prioritize Context:** You MUST base all plant facts, recommendations, and data **directly** on the provided `Plant Corpus` context. Do not use your general knowledge if it conflicts with the context.\n",
    "\n",
    "2.  **Synthesize, Don't Just Repeat:** When answering, intelligently synthesize information from the context's fields. For example, to answer \"what plant should I grow,\" you MUST combine `Climate Requirements`, `Soil Type`, and `Water Needs` to explain *why* a plant is suitable.\n",
    "\n",
    "3.  **Handle Missing Information (Critical):** If the user asks about a plant, region, or topic that is **not** in the provided context, you MUST clearly state that the information is not available in your specialized corpus. However, you can use your general knowledge to invent an answer for a plant not in list.\n",
    "\n",
    "4.  **Use Your Persona:** After you have provided the core facts *from the corpus*, you may add a brief, practical tip based on your persona (e.g., \"In my experience on the plains, this plant also helps with soil erosion,\" or \"Be sure to protect it from...\").\n",
    "\n",
    "5.  **Corpus Structure (For Your Reference):**\n",
    "    `Plant ID`, `Common Name`, `Scientific Name`, `Local Name (If Applicable)`, `Region`, `Climate Requirements`, `Soil Type`, `Sun Light Needs`, `Water Needs`, `Growth Rate`, `Ecological Role`, `Traditional Uses`\n",
    "\"\"\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    user_query = request.state[\"messages\"][-1].text\n",
    "    corpus_context = vector_store.similarity_search(query=user_query)\n",
    "\n",
    "    corpus_content = \"\\n\".join(doc.page_content for doc in corpus_context)\n",
    "\n",
    "    system_message = SYSTEM_PROMPT_TEMPLATE.format(corpus_content=corpus_content)\n",
    "    return system_message\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context]) \n",
    "\n",
    "noRAG_agent = create_agent(model, tools=[], middleware=[]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bbd5a4-be47-4f1a-aecf-5a9f607a8dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 25 documents into ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "df_corpus = pd.read_csv(\"Corpus.csv\")\n",
    "df_corpus = df_corpus.fillna(\"NaN\")\n",
    "df_corpus.to_csv(\"CleanCorpus.csv\")\n",
    "\n",
    "openAI_embed = OpenAIEmbeddings(model=\"text-embedding-3-large\") \n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"CleanCorpus.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=openAI_embed,\n",
    "    collection_name=\"plantCorpus_langchain\"\n",
    ")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents into ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7a119-9d07-4314-af9d-795b07379876",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_personas = {\n",
    "    \"Evaluator_1_Landscape_Architect\": (\n",
    "        \"You are a Senior Landscape Architect for US Embassies. \"\n",
    "        \"You prioritize aesthetics, formal structure, and low maintenance. \"\n",
    "        \"You are strict about visual appeal and infrastructure safety.\"\n",
    "    ),\n",
    "    \"Evaluator_2_Local_Botanist\": (\n",
    "        \"You are a PhD Botanist specializing in the Indo-Gangetic Plain. \"\n",
    "        \"You care deeply about scientific accuracy, correct Latin names, \"\n",
    "        \"and specific soil/climate requirements. You dislike vague answers.\"\n",
    "    ),\n",
    "    \"Evaluator_3_Cultural_Historian\": (\n",
    "        \"You are an expert in Indian Ethnobotany and Folklore. \"\n",
    "        \"You focus on cultural relevance, traditional uses (Ayurveda), \"\n",
    "        \"and local naming conventions. You want to see cultural depth.\"\n",
    "    ),\n",
    "    \"Evaluator_4_Sustainability_Officer\": (\n",
    "        \"You are a Sustainability Officer focused on water conservation. \"\n",
    "        \"You heavily penalize plants that require too much water or fertilizer. \"\n",
    "        \"You prioritize ecological suitability and native species.\"\n",
    "    ),\n",
    "    \"Evaluator_5_General_Resident\": (\n",
    "        \"You are a homeowner in the region with average gardening skills. \"\n",
    "        \"You care about 'Overall Helpfulness' and simple, clear advice. \"\n",
    "        \"You find overly technical jargon unhelpful.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def get_evaluation(judge_persona, user_query, model_response):\n",
    "    system_prompt = f\"\"\"\n",
    "    {judge_persona}\n",
    "    \n",
    "    You are evaluating an AI assistant's response to a user query about plants in the Indo-Gangetic Plains.\n",
    "    \n",
    "    Please rate the response on these 4 metrics using a 1-5 Likert Scale:\n",
    "    1. Factual Accuracy (1=Hallucinated/Wrong, 5=Highly Accurate/Cited)\n",
    "    2. Ecological Suitability (1=Invasive/Deadly, 5=Perfect for Region)\n",
    "    3. Cultural Relevance (1=Generic/Ignorant, 5=Culturally Insightful/Local Context)\n",
    "    4. Overall Helpfulness (1=Useless, 5=Very Helpful)\n",
    "    \n",
    "    RETURN JSON ONLY in this format:\n",
    "    {{\n",
    "        \"Factual_Accuracy\": int,\n",
    "        \"Ecological_Suitability\": int,\n",
    "        \"Cultural_Relevance\": int,\n",
    "        \"Overall_Helpfulness\": int,\n",
    "        \"Reasoning\": \"Short explanation (max 1 sentence)\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    user_content = f\"\"\"USER QUERY = {user_query}\n",
    "                    AI RESPONSE = {model_response}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_content)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(messages)\n",
    "        content_str = response.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(content_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820e622c-71e6-4629-b2e8-463790ed477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: rag.json or norag.json not found. Evaluation loop will fail.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"rag.json\", 'r', encoding=\"utf-8\") as rag_file:\n",
    "        rag_data = json.load(rag_file)\n",
    "    with open(\"norag.json\", 'r', encoding=\"utf-8\") as norag_file:\n",
    "        norag_data = json.load(norag_file)\n",
    "    print(f\"Loaded {len(rag_data)} RAG responses and {len(norag_data)} NoRAG responses.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"rag.json or norag.json not found\")\n",
    "    rag_data, norag_data = [], []\n",
    "\n",
    "results = []\n",
    "if rag_data and norag_data:\n",
    "    for i, (rag_item, norag_item) in enumerate(zip(rag_data, norag_data)):\n",
    "        query = rag_item['query']\n",
    "        rag_response = rag_item['response']\n",
    "        norag_response = norag_item['response']\n",
    "\n",
    "        if rag_item['query'] != norag_item['query']:\n",
    "            print(f\"Warning: Mismatch in queries at index {i}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Evaluating Query {i+1}/{len(rag_data)}\")\n",
    "\n",
    "        for evaluator_name, persona_prompt in evaluator_personas.items():\n",
    "            rag_grade = get_evaluation(persona_prompt, query, rag_response)\n",
    "            if rag_grade:\n",
    "                results.append({\n",
    "                    \"Query_ID\": i+1,\n",
    "                    \"System\": \"RAG\",\n",
    "                    \"Evaluator\": evaluator_name,\n",
    "                    **rag_grade \n",
    "                })\n",
    "            \n",
    "            norag_grade = get_evaluation(persona_prompt, query, norag_response)\n",
    "            if norag_grade:\n",
    "                results.append({\n",
    "                    \"Query_ID\": i+1,\n",
    "                    \"System\": \"No-RAG\",\n",
    "                    \"Evaluator\": evaluator_name,\n",
    "                    **norag_grade \n",
    "                })\n",
    "        time.sleep(1)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"evaluation_results.csv\", index=False)\n",
    "    print(\"Evaluation Complete. Saved to evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5c638-8899-4042-ace0-10758b150d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INTER-RATER RELIABILITY (Krippendorff's Alpha) ===\n",
      "Factual_Accuracy         : 0.7535\n",
      "Ecological_Suitability   : 0.8050\n",
      "Cultural_Relevance       : 0.8399\n",
      "Overall_Helpfulness      : 0.8260\n",
      "\n",
      "=== BERTScore (Semantic Divergence) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562acbf38c814f888b5e5d69c109e6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.read_csv(\"evaluation_results.csv\")\n",
    "with open(\"ragList.json\", 'r', encoding=\"utf-8\") as rag_file:\n",
    "        rag_data = json.load(rag_file)\n",
    "with open(\"nonragList.json\", 'r', encoding=\"utf-8\") as norag_file:\n",
    "        norag_data = json.load(norag_file)\n",
    "\n",
    "# Krippendorff's Alpha\n",
    "if not df_results.empty:\n",
    "    print(\"\\n=== INTER-RATER RELIABILITY (Krippendorff's Alpha) ===\")\n",
    "    metrics = [\"Factual_Accuracy\", \"Ecological_Suitability\", \"Cultural_Relevance\", \"Overall_Helpfulness\"]\n",
    "    df_results['Response_ID'] = df_results['Query_ID'].astype(str) + \"_\" + df_results['System']\n",
    "\n",
    "    for metric in metrics:\n",
    "        try:\n",
    "            pivot_table = df_results.pivot(index='Response_ID', columns='Evaluator', values=metric)\n",
    "            reliability_data = pivot_table.transpose().to_numpy()\n",
    "            alpha = krippendorff.alpha(reliability_data=reliability_data, level_of_measurement='ordinal')\n",
    "            print(f\"{metric:<25}: {alpha:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate alpha for {metric}: {e}\")\n",
    "\n",
    "# BERTScore\n",
    "df_rag = pd.DataFrame(rag_data)\n",
    "df_norag = pd.DataFrame(norag_data)\n",
    "print(\"\\n=== BERTScore (Semantic Divergence) ===\")\n",
    "if rag_data and norag_data:\n",
    "    df_rag = pd.DataFrame(rag_data)\n",
    "    df_norag = pd.DataFrame(norag_data)\n",
    "\n",
    "    P, R, F1 = bert_score(\n",
    "        df_rag['response'].to_list(),\n",
    "        df_norag['response'].to_list(),\n",
    "        lang=\"eng\",\n",
    "        verbose=False\n",
    "    )\n",
    "    df_rag['BERTScore_Similarity'] = F1.numpy()\n",
    "    print(f\"Avg Similarity to Base Model: {df_rag['BERTScore_Similarity'].mean():.3f}\")\n",
    "\n",
    "# RAGAS\n",
    "print(\"\\n=== RAGAS (Answer Relevancy) ===\")\n",
    "if rag_data:\n",
    "    gpt4_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    ragas_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    ragas_dataset = Dataset.from_dict({\n",
    "        'question': df_rag['query'].tolist(),\n",
    "        'answer': df_rag['response'].tolist(),\n",
    "        'contexts': [[''] for _ in range(len(df_rag))]\n",
    "    })\n",
    "\n",
    "    ragas_results = evaluate(\n",
    "        dataset=ragas_dataset, \n",
    "        metrics=[answer_relevancy], \n",
    "        llm=gpt4_llm, \n",
    "        embeddings=ragas_embeddings\n",
    "    )\n",
    "\n",
    "    df_rag['Ragas_Relevancy'] = ragas_results['answer_relevancy']\n",
    "    print(f\"Avg Relevancy Score: {df_rag['Ragas_Relevancy'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f689539-d01d-4cbe-8f17-50dcf84e4a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
