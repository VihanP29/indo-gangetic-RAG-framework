{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dca4aa-3129-49b2-98da-766e5d73a82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8155bb40-ebd3-467b-90a8-e8e815a147d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# RAG & LangChain Imports\n",
    "import chromadb\n",
    "import langchain\n",
    "import langchainhub\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Evaluation Metrics Imports\n",
    "import krippendorff\n",
    "from bert_score import score as bert_score\n",
    "from ragas.metrics import answer_relevancy\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749286a-1690-4e5b-8f08-42afb623c5a5",
   "metadata": {},
   "source": [
    "### RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89171057-9465-4cd4-974b-06f4875f7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "openAI_embed = OpenAIEmbeddings(model=\"text-embedding-3-large\") #\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"CleanCorpus.csv\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Ingest into Chroma (using LangChain wrapper)\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=openAI_embed,\n",
    "    collection_name=\"plantCorpus_langchain\"\n",
    ") #\n",
    "\n",
    "# B. Define Dynamic Prompt Middleware\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "## Role and Goal\n",
    "You are an expert landscaping and agricultural assistant with deep, practical knowledge of the Indo-Gangetic plains. Your goal is to provide accurate, specific answers to user queries about plants.\n",
    "\n",
    "## Source of Truth: Plant Corpus\n",
    "You will be given context from a specialized `Plant Corpus`. This corpus is your **single source of truth** for all plant-specific data.\n",
    "\n",
    "### Retrieved Context:\n",
    "{corpus_content}\n",
    "\n",
    "## Core Instructions and Rules\n",
    "\n",
    "1.  **Prioritize Context:** You MUST base all plant facts, recommendations, and data **directly** on the provided `Plant Corpus` context. Do not use your general knowledge if it conflicts with the context.\n",
    "\n",
    "2.  **Synthesize, Don't Just Repeat:** When answering, intelligently synthesize information from the context's fields. For example, to answer \"what plant should I grow,\" you MUST combine `Climate Requirements`, `Soil Type`, and `Water Needs` to explain *why* a plant is suitable.\n",
    "\n",
    "3.  **Handle Missing Information (Critical):** If the user asks about a plant, region, or topic that is **not** in the provided context, you MUST clearly state that the information is not available in your specialized corpus. However, you can use your general knowledge to invent an answer for a plant not in list.\n",
    "\n",
    "4.  **Use Your Persona:** After you have provided the core facts *from the corpus*, you may add a brief, practical tip based on your persona (e.g., \"In my experience on the plains, this plant also helps with soil erosion,\" or \"Be sure to protect it from...\").\n",
    "\n",
    "5.  **Corpus Structure (For Your Reference):**\n",
    "    `Plant ID`, `Common Name`, `Scientific Name`, `Local Name (If Applicable)`, `Region`, `Climate Requirements`, `Soil Type`, `Sun Light Needs`, `Water Needs`, `Growth Rate`, `Ecological Role`, `Traditional Uses`\n",
    "\"\"\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    user_query = request.state[\"messages\"][-1].text\n",
    "    corpus_context = vector_store.similarity_search(query=user_query)\n",
    "\n",
    "    corpus_content = \"\\n\".join(doc.page_content for doc in corpus_context)\n",
    "\n",
    "    system_message = SYSTEM_PROMPT_TEMPLATE.format(corpus_content=corpus_content)\n",
    "    return system_message\n",
    "\n",
    "# C. Initialize Models & Agents\n",
    "model = init_chat_model(\"gpt-4.1\") # Note: Ensure this model name is valid in your API, otherwise use \"gpt-4o\" or \"gpt-4-turbo\"\n",
    "\n",
    "# The RAG Agent (Uses the middleware)\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context]) \n",
    "\n",
    "# The Baseline Agent (No RAG, no middleware)\n",
    "noRAG_agent = create_agent(model, tools=[], middleware=[]) \n",
    "\n",
    "print(\"RAG System Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbd5a4-be47-4f1a-aecf-5a9f607a8dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
